{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.metrics import accuracy_score\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from more_itertools import set_partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'eye-color.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path, sep=',')\n",
    "X_set = df.iloc[:,:-1]\n",
    "y_set = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eye</th>\n",
       "      <th>Hair</th>\n",
       "      <th>Height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Blue</td>\n",
       "      <td>Blonde</td>\n",
       "      <td>Tall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Blue</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brown</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Green</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Green</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Tall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Brown</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Green</td>\n",
       "      <td>Blonde</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Blue</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Eye    Hair  Height\n",
       "0   Blue  Blonde    Tall\n",
       "1   Blue   Brown  Medium\n",
       "2  Brown   Brown  Medium\n",
       "3  Green   Brown  Medium\n",
       "4  Green   Brown    Tall\n",
       "5  Brown   Brown     Low\n",
       "6  Green  Blonde     Low\n",
       "7   Blue   Brown  Medium"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= df\n",
    "header = list(dataset)\n",
    "dataset = dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Blue', 'Blonde', 'Tall', 'C+'],\n",
       "       ['Blue', 'Brown', 'Medium', 'C+'],\n",
       "       ['Brown', 'Brown', 'Medium', 'C-'],\n",
       "       ['Green', 'Brown', 'Medium', 'C-'],\n",
       "       ['Green', 'Brown', 'Tall', 'C+'],\n",
       "       ['Brown', 'Brown', 'Low', 'C-'],\n",
       "       ['Green', 'Blonde', 'Low', 'C-'],\n",
       "       ['Blue', 'Brown', 'Medium', 'C+']], dtype=object)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training instances: 8\n",
      "Number of test instances: 8\n"
     ]
    }
   ],
   "source": [
    "num_rows, num_cols = np.shape(dataset)\n",
    "num_test_rows = num_rows\n",
    "num_training_rows = num_rows\n",
    "\n",
    "num_cols = num_cols -1\n",
    "class_index = num_cols\n",
    "feature_indices = list(range(class_index)) + list(range(class_index + 1, num_cols))\n",
    "\n",
    "training = dataset[num_test_rows:]\n",
    "test = dataset[:num_test_rows]\n",
    "print('Number of training instances: ' + str(num_training_rows))\n",
    "print('Number of test instances: ' + str(num_test_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Aggregate all the attribute values\n",
    "attribute_values = [None] * num_cols\n",
    "attribute_indices = [None] * num_cols\n",
    "for attribute_index in feature_indices:\n",
    "    attribute_values[attribute_index] = list(set([x[attribute_index] for x in dataset]))\n",
    "    attribute_indices[attribute_index] = list(range(len(attribute_values[attribute_index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Green', 'Blue', 'Brown'], ['Blonde', 'Brown'], ['Medium', 'Tall', 'Low']]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribute_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Green', 'Blue', 'Brown'], ['Blonde', 'Brown'], ['Medium', 'Tall', 'Low']]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribute_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2], [0, 1], [0, 1, 2]]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attribute_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attribute_subsets(subset, attribute_index):\n",
    "    attribute_subsets = {}\n",
    "    for sample in subset:\n",
    "        if sample[attribute_index] in attribute_subsets:\n",
    "            attribute_subsets[sample[attribute_index]].append(sample)\n",
    "        else:\n",
    "            attribute_subsets[sample[attribute_index]] = [sample]\n",
    "    return attribute_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def build_tree(subset, unused_attributes, feature_subset_size):\n",
    "\n",
    "subset, unused_attributes, feature_subset_size = test, feature_indices, len(feature_indices)\n",
    "\n",
    "# If all elements of subset are in the same class we can make this a leaf node\n",
    "if len(set([x[class_index] for x in subset])) == 1:\n",
    "    tree = subset[0][class_index]\n",
    "\n",
    "if len(unused_attributes) == 0:\n",
    "    # Get most common class\n",
    "    tree = get_most_common_class(subset)\n",
    "\n",
    "# Apply the random forest technique of only considering a subset of features here\n",
    "# but only if we have enough features to begin with.\n",
    "feature_subset = unused_attributes\n",
    "if len(feature_subset) > feature_subset_size:\n",
    "    feature_subset = random.sample(feature_subset, feature_subset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Blue', 'Brown', 'Green'}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(attribute_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.033333333333333326\n",
      "0.16666666666666669\n",
      "0.3000000000000001\n",
      "0.0\n",
      "0.0\n",
      "0.16666666666666669\n",
      "0.16666666666666669\n"
     ]
    }
   ],
   "source": [
    "best_split_gini = 10\n",
    "attribute_index = None\n",
    "best_split_value = None\n",
    "best_split_groups  = None\n",
    "gini_X = single_gini_index(df)\n",
    "for attribute_index in feature_subset:\n",
    "    partitions = list(set_partitions(attribute_values[attribute_index], 2))\n",
    "    for part in partitions:\n",
    "        left_split, right_split = build_split(df, attribute_index, part)\n",
    "        gini_XA =  multi_gini_index(left_split, right_split)\n",
    "       ob gini_A = gini_X - gini_XA\n",
    "        print(gini_A)\n",
    "        if gini_XA < best_split_gini:\n",
    "            best_split_gini = gini_XA\n",
    "            best_split_column  = attribute_index\n",
    "            best_split_value = part\n",
    "            best_split_groups = left_split, right_split\n",
    "return_dict = {'column_name': best_split_column,'dsplit_value':best_split_value,\n",
    "             'gini':best_split_gini, 'groups': best_split_groups}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Blue'], ['Green', 'Brown']]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_split_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eye</th>\n",
       "      <th>Hair</th>\n",
       "      <th>Height</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Blue</td>\n",
       "      <td>Blonde</td>\n",
       "      <td>Tall</td>\n",
       "      <td>C+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Blue</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "      <td>C+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Blue</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "      <td>C+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Eye    Hair  Height class\n",
       "0  Blue  Blonde    Tall    C+\n",
       "1  Blue   Brown  Medium    C+\n",
       "7  Blue   Brown  Medium    C+"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_split_groups[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eye</th>\n",
       "      <th>Hair</th>\n",
       "      <th>Height</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brown</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "      <td>C-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Green</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "      <td>C-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Green</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Tall</td>\n",
       "      <td>C+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Brown</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Low</td>\n",
       "      <td>C-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Green</td>\n",
       "      <td>Blonde</td>\n",
       "      <td>Low</td>\n",
       "      <td>C-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Eye    Hair  Height class\n",
       "2  Brown   Brown  Medium    C-\n",
       "3  Green   Brown  Medium    C-\n",
       "4  Green   Brown    Tall    C+\n",
       "5  Brown   Brown     Low    C-\n",
       "6  Green  Blonde     Low    C-"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[header[0]].isin(['Brown', 'Green'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_split(data,column_to_split,split_values):\n",
    "    '''build 2 groups of data by splitting data on the column_to_split \n",
    "       at the split_value'''\n",
    "\n",
    "    left_split = data.loc[data[header[column_to_split]].isin(split_values[0])]\n",
    "    right_split = data.loc[data[header[column_to_split]].isin(split_values[1])]\n",
    "    \n",
    "    return left_split,right_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_gini_index(group1,group2):\n",
    "    '''Calculate Gini Impurity, func expects to be passed \n",
    "       the 2 groups of data that are the result of a split'''\n",
    "    class_proportions_group1 = group1['class'].value_counts(normalize=True)    \n",
    "    class_proportions_group2 = group2['class'].value_counts(normalize=True)    \n",
    "\n",
    "    instance_proportion_group1 = len(group1)/(len(group1)+len(group2))\n",
    "    instance_proportion_group2 = len(group2)/(len(group1)+len(group2))\n",
    "\n",
    "    gini1 = (1 - class_proportions_group1.pow(2).sum())*(instance_proportion_group1)\n",
    "    gini2 = (1 - class_proportions_group2.pow(2).sum())*(instance_proportion_group2)\n",
    "    gini = gini1+gini2\n",
    "\n",
    "    return gini\n",
    "\n",
    "def single_gini_index(group):\n",
    "    '''Calculate Gini Impurity of a single group'''\n",
    "    class_proportions = group['class'].value_counts(normalize=True)    \n",
    "\n",
    "    gini = (1 - class_proportions.pow(2).sum())\n",
    "  \n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eye</th>\n",
       "      <th>Hair</th>\n",
       "      <th>Height</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brown</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "      <td>C-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Green</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "      <td>C-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Green</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Tall</td>\n",
       "      <td>C+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Brown</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Low</td>\n",
       "      <td>C-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Green</td>\n",
       "      <td>Blonde</td>\n",
       "      <td>Low</td>\n",
       "      <td>C-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Eye    Hair  Height class\n",
       "2  Brown   Brown  Medium    C-\n",
       "3  Green   Brown  Medium    C-\n",
       "4  Green   Brown    Tall    C+\n",
       "5  Brown   Brown     Low    C-\n",
       "6  Green  Blonde     Low    C-"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_split_groups[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Green', 'Brown']\n",
      "[[['Green'], ['Brown']]]\n",
      "0.26666666666666666\n",
      "['Blonde', 'Brown']\n",
      "[[['Blonde'], ['Brown']]]\n",
      "0.30000000000000004\n",
      "['Medium', 'Tall', 'Low']\n",
      "[[['Medium'], ['Tall', 'Low']], [['Medium', 'Tall'], ['Low']], [['Tall'], ['Medium', 'Low']]]\n",
      "0.26666666666666666\n",
      "0.26666666666666666\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'column_name': 2,\n",
       " 'dsplit_value': [['Tall'], ['Medium', 'Low']],\n",
       " 'gini': 0.31999999999999984,\n",
       " 'groups': (     Eye   Hair Height class\n",
       "  4  Green  Brown   Tall    C+,\n",
       "       Eye    Hair  Height class\n",
       "  2  Brown   Brown  Medium    C-\n",
       "  3  Green   Brown  Medium    C-\n",
       "  5  Brown   Brown     Low    C-\n",
       "  6  Green  Blonde     Low    C-)}"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_best_split_point(best_split_groups[1], feature_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "types=list(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "types[0] == 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split_point(passed_data, feature_subset):\n",
    "    '''find best split point iterating over range of values returned from the \n",
    "    get_range_to_split_on function and return a dictionary which functions as a node '''\n",
    "\n",
    "    best_split_gini = 10\n",
    "    attribute_index = None\n",
    "    best_split_value = None\n",
    "    best_split_groups  = None\n",
    "    best_split_column = None\n",
    "    gini_X = single_gini_index(passed_data)\n",
    "    for attribute_index in feature_subset:\n",
    "        \n",
    "        attribute_values = list(set([x[attribute_index] for x in passed_data.values]))\n",
    "        if len(attribute_values) == 1:\n",
    "            gini_XA = single_gini_index(passed_data)\n",
    "            if gini_XA < best_split_gini:\n",
    "                best_split_gini = gini_XA\n",
    "                best_split_column  = attribute_index\n",
    "                best_split_value = attribute_values\n",
    "                best_split_groups = passed_data, pd.DataFrame(columns = passed_data.columns)\n",
    "        else:    \n",
    "            partitions = list(set_partitions(attribute_values, 2))\n",
    "            for part in partitions:\n",
    "                if len(part[1]) < len(part[0]):\n",
    "                    part = [part[1], part[0]]    \n",
    "                left_split, right_split = build_split(passed_data, attribute_index, part)\n",
    "                gini_XA =  multi_gini_index(left_split, right_split)\n",
    "                gini_A = gini_X - gini_XA\n",
    "                if gini_XA < best_split_gini:\n",
    "                    best_split_gini = gini_XA\n",
    "                    best_split_column  = attribute_index\n",
    "                    best_split_value = part\n",
    "                    best_split_groups = left_split, right_split\n",
    "    return {'column_name': best_split_column,'dsplit_value':best_split_value,\n",
    "                 'gini':best_split_gini, 'groups': best_split_groups}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = find_best_split_point(df, [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'column_name': 0,\n",
       " 'dsplit_value': [['Blue'], ['Green', 'Brown']],\n",
       " 'gini': 0.1999999999999999,\n",
       " 'groups': (    Eye    Hair  Height class\n",
       "  0  Blue  Blonde    Tall    C+\n",
       "  1  Blue   Brown  Medium    C+\n",
       "  7  Blue   Brown  Medium    C+,\n",
       "       Eye    Hair  Height class\n",
       "  2  Brown   Brown  Medium    C-\n",
       "  3  Green   Brown  Medium    C-\n",
       "  4  Green   Brown    Tall    C+\n",
       "  5  Brown   Brown     Low    C-\n",
       "  6  Green  Blonde     Low    C-)}"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_splitter(node,random_features):\n",
    "    '''this function recursively splits the data starting with the root node which its passed\n",
    "    untill the groups are homogenous or further splits result in empty nodes'''\n",
    "    left_group,right_group = node['groups']\n",
    "    #delete the groups entry in original node\n",
    "    del node['groups']\n",
    "    #check if the groups of the node are empty\n",
    "    if left_group.empty or right_group.empty:\n",
    "        #combine as we will use original to predict\n",
    "        combined = pd.concat([left_group,right_group])\n",
    "        predicted_class = combined['class'].value_counts().index[0]\n",
    "        node['left']=node['right']=predicted_class\n",
    "        return [predicted_class]\n",
    "    #check if the groups of the node are homogenous otherwise call recursive_spltter again\n",
    "    if single_gini_index(left_group) == 0:\n",
    "        predicted_class = left_group['class'].value_counts().index[0]\n",
    "        node['left'] = predicted_class\n",
    "    else:\n",
    "        node['left'] = find_best_split_point(left_group,random_features)\n",
    "        curr_node = recursive_splitter(node['left'],random_features)\n",
    "        if type(curr_node) == list:\n",
    "            node['left'] = curr_node[0]\n",
    "\n",
    "    if single_gini_index(right_group) == 0:\n",
    "        predicted_class = right_group['class'].value_counts().index[0]\n",
    "        node['right'] = predicted_class\n",
    "    else:\n",
    "        node['right'] = find_best_split_point(right_group,random_features)\n",
    "        curr_node = recursive_splitter(node['right'],random_features)\n",
    "        if type(curr_node) == list:\n",
    "            node['right'] = curr_node[0]        \n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eye</th>\n",
       "      <th>Hair</th>\n",
       "      <th>Height</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Blue</td>\n",
       "      <td>Blonde</td>\n",
       "      <td>Tall</td>\n",
       "      <td>C+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Blue</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "      <td>C+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brown</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "      <td>C-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Green</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "      <td>C-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Green</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Tall</td>\n",
       "      <td>C+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Brown</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Low</td>\n",
       "      <td>C-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Green</td>\n",
       "      <td>Blonde</td>\n",
       "      <td>Low</td>\n",
       "      <td>C-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Blue</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Medium</td>\n",
       "      <td>C+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Eye    Hair  Height class\n",
       "0   Blue  Blonde    Tall    C+\n",
       "1   Blue   Brown  Medium    C+\n",
       "2  Brown   Brown  Medium    C-\n",
       "3  Green   Brown  Medium    C-\n",
       "4  Green   Brown    Tall    C+\n",
       "5  Brown   Brown     Low    C-\n",
       "6  Green  Blonde     Low    C-\n",
       "7   Blue   Brown  Medium    C+"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a) == dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = recursive_splitter(nn, [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'column_name': 0,\n",
       " 'dsplit_value': [['Blue'], ['Green', 'Brown']],\n",
       " 'gini': 0.1999999999999999,\n",
       " 'left': 'C+',\n",
       " 'right': {'column_name': 0,\n",
       "  'dsplit_value': [['Green'], ['Brown']],\n",
       "  'gini': 0.26666666666666666,\n",
       "  'left': {'column_name': 1,\n",
       "   'dsplit_value': [['Blonde'], ['Brown']],\n",
       "   'gini': 0.3333333333333333,\n",
       "   'left': 'C-',\n",
       "   'right': 'C+'},\n",
       "  'right': 'C-'}}"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = ['Green', 'Brown', 'Tall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_prediction_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr[0] in a['dsplit_value'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C+'"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_prediction_tree(arr, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction_tree(data_row,root_node):\n",
    "    '''recursively traverse the tree from root to leaf turning left if feature value\n",
    "    to test is less than dsplit_value or right otherwise until we reach a leaf node'''\n",
    "    \n",
    "    #check if feature of data_row is less than dsplit_value else move to right branch\n",
    "    if data_row[root_node['column_name']] in root_node['dsplit_value'][0]:\n",
    "        #check if at a branch or a leaf if branch recursively call predict else return leaf prediction\n",
    "        if type(root_node['left']) is dict:\n",
    "            return make_prediction_tree(data_row,root_node['left'])\n",
    "        else:\n",
    "            return root_node['left']\n",
    "    else:\n",
    "        if type(root_node['right']) is dict:\n",
    "            return make_prediction_tree(data_row,root_node['right'])\n",
    "        else:\n",
    "            return root_node['right']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "min_index, min_entropy = 0, sys.maxsize\n",
    "for attribute_index in feature_subset:\n",
    "    # Create subsets for that given attribute\n",
    "    attribute_subsets = create_attribute_subsets(subset, attribute_index)\n",
    "\n",
    "    # Calculate entropy of the generated subsets and weight them according to their size\n",
    "    entropy = 0\n",
    "    for attribute_subset in attribute_subsets.values():\n",
    "        entropy += float(len(attribute_subset) / len(subset)) * calculate_entropy(attribute_subset)\n",
    "\n",
    "    # Keep track of the split attribute with minimum entropy\n",
    "    if entropy < min_entropy:\n",
    "        min_index, min_entropy = attribute_index, entropy\n",
    "\n",
    "# Copy list otherwise our passed feature_indices array will be modified.\n",
    "unused_attributes = list(unused_attributes)\n",
    "unused_attributes.remove(min_index)\n",
    "\n",
    "# Reconstruct subsets of best attribute and use them to build subtrees recursively\n",
    "# As a measure of feature importance we save the feature importance (information gain) here\n",
    "attribute_subsets = create_attribute_subsets(subset, min_index)\n",
    "tree = {'attribute_index': min_index, 'subtrees': {}, 'information_gain': calculate_entropy(subset) - min_entropy}\n",
    "\n",
    "# Make sure to add for all values that exist in the training dataset labels\n",
    "most_common_class = None\n",
    "for attribute_value in attribute_values[min_index]:\n",
    "    if attribute_value in attribute_subsets and len(attribute_subsets[attribute_value]):\n",
    "        tree['subtrees'][attribute_value] = build_tree(attribute_subsets[attribute_value], unused_attributes,\n",
    "                                                       feature_subset_size)\n",
    "    else:\n",
    "        if not most_common_class:\n",
    "            most_common_class = get_most_common_class(subset)\n",
    "        tree['subtrees'][attribute_value] = most_common_class\n",
    "\n",
    "#return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split_point(passed_data, random_features):\n",
    "    '''find best split point iterating over range of values returned from the \n",
    "    get_range_to_split_on function and return a dictionary which functions as a node '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_class_counts(subset):\n",
    "    class_counts = {}\n",
    "    for sample in subset:\n",
    "        if sample[class_index] in class_counts:\n",
    "            class_counts[sample[class_index]] += 1\n",
    "        else:\n",
    "            class_counts[sample[class_index]] = 1\n",
    "    return class_counts\n",
    "\n",
    "\n",
    "def get_most_common_class(subset):\n",
    "    max_class_count, most_common_class = -1, ''\n",
    "    class_counts = create_class_counts(subset)\n",
    "    for the_class, class_count in class_counts.items():\n",
    "        if class_count > max_class_count:\n",
    "            max_class_count, most_common_class = class_count, the_class\n",
    "    return most_common_class\n",
    "\n",
    "\n",
    "def calculate_entropy(subset):\n",
    "    class_counts = create_class_counts(subset)\n",
    "    subset_entropy = 0\n",
    "    for class_count in class_counts.values():\n",
    "        p_x = float(class_count) / len(subset)\n",
    "        subset_entropy -= p_x * math.log(p_x)\n",
    "    return subset_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split_point(passed_data, random_features):\n",
    "    '''find best split point iterating over range of values returned from the \n",
    "    get_range_to_split_on function and return a dictionary which functions as a node '''\n",
    "\n",
    "    \n",
    "     #intialise values for best split point\n",
    "    best_split_column = 'name'\n",
    "    best_split_value = 0\n",
    "    best_split_gini = 10\n",
    "    best_split_groups = None\n",
    "    \n",
    "    #iterate over columns and rows searching for best split point\n",
    "    for col_name in random_features:\n",
    "        # for split_value in splitpoints_dict[col_name]:\n",
    "        for index,row in passed_data.iterrows():\n",
    "            left_split, right_split = build_split(passed_data,col_name,row[col_name])\n",
    "            gini_score = multi_gini_index(left_split, right_split)\n",
    "\n",
    "            if gini_score < best_split_gini:\n",
    "                best_split_gini = gini_score\n",
    "                best_split_column = col_name\n",
    "                best_split_value = row[col_name]\n",
    "                best_split_groups = left_split, right_split\n",
    "    \n",
    "    \n",
    "    #print(best_split_column,best_split_gini)\n",
    "    return {'column_name': best_split_column,'dsplit_value':best_split_value,\n",
    "             'gini':best_split_gini, 'groups': best_split_groups}\n",
    "\n",
    "def build_split(data,column_to_split,split_value):\n",
    "    '''build 2 groups of data by splitting data on the column_to_split \n",
    "       at the split_value'''\n",
    "    left_split = data[data[column_to_split]<split_value]\n",
    "    right_split = data[data[column_to_split]>=split_value]\n",
    "    \n",
    "    return left_split,right_split\n",
    "\n",
    "    \n",
    "def multi_gini_index(group1,group2):\n",
    "    '''Calculate Gini Impurity, func expects to be passed \n",
    "       the 2 groups of data that are the result of a split'''\n",
    "    class_proportions_group1 = group1['class'].value_counts(normalize=True)    \n",
    "    class_proportions_group2 = group2['class'].value_counts(normalize=True)    \n",
    "\n",
    "    instance_proportion_group1 = len(group1)/(len(group1)+len(group2))\n",
    "    instance_proportion_group2 = len(group2)/(len(group1)+len(group2))\n",
    "\n",
    "    gini1 = (1 - class_proportions_group1.pow(2).sum())*(instance_proportion_group1)\n",
    "    gini2 = (1 - class_proportions_group2.pow(2).sum())*(instance_proportion_group2)\n",
    "    gini = gini1+gini2\n",
    "\n",
    "    return gini\n",
    "\n",
    "def single_gini_index(group):\n",
    "    '''Calculate Gini Impurity of a single group'''\n",
    "    class_proportions = group['class'].value_counts(normalize=True)    \n",
    "\n",
    "    gini = (1 - class_proportions.pow(2).sum())\n",
    "  \n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plant_tree(train_data, number_features=3,num_bins=10):\n",
    "    ''' start the process of recursion on the training data and let the tree\n",
    "    grow to its max depth using subset of random features'''\n",
    "      \n",
    "    #get column names minus class\n",
    "    #choose random set of features from column names\n",
    "    data_column_names = list(train_data.columns)\n",
    "    data_column_names.remove('class')\n",
    "    random_features = random.sample(data_column_names,number_features)\n",
    "\n",
    "    root_node = find_best_split_point(train_data,random_features)\n",
    "    recursive_spltter(root_node,random_features)\n",
    "    return root_node,random_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, predicted_class):\n",
    "        self.predicted_class = predicted_class\n",
    "        self.feature_index = 0\n",
    "        self.threshold = 0\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.n_classes_ = len(set(y))\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.tree_ = self._grow_tree(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return [self._predict(inputs) for inputs in X]\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        m = y.size\n",
    "        if m <= 1:\n",
    "            return None, None\n",
    "        num_parent = [np.sum(y == c) for c in range(self.n_classes_)]\n",
    "        best_gini = 1.0 - sum((n / m) ** 2 for n in num_parent)\n",
    "        best_idx, best_thr = None, None\n",
    "        for idx in range(self.n_features_):\n",
    "            thresholds, classes = zip(*sorted(zip(X[:, idx], y)))\n",
    "            num_left = [0] * self.n_classes_\n",
    "            num_right = num_parent.copy()\n",
    "            for i in range(1, m):\n",
    "                c = classes[i - 1]\n",
    "                num_left[c] += 1\n",
    "                num_right[c] -= 1\n",
    "                gini_left = 1.0 - sum(\n",
    "                    (num_left[x] / i) ** 2 for x in range(self.n_classes_)\n",
    "                )\n",
    "                gini_right = 1.0 - sum(\n",
    "                    (num_right[x] / (m - i)) ** 2 for x in range(self.n_classes_)\n",
    "                )\n",
    "                gini = (i * gini_left + (m - i) * gini_right) / m\n",
    "                if thresholds[i] == thresholds[i - 1]:\n",
    "                    continue\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_idx = idx\n",
    "                    best_thr = (thresholds[i] + thresholds[i - 1]) / 2\n",
    "        return best_idx, best_thr\n",
    "\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]\n",
    "        predicted_class = np.argmax(num_samples_per_class)\n",
    "        node = Node(predicted_class=predicted_class)\n",
    "        if depth < self.max_depth:\n",
    "            idx, thr = self._best_split(X, y)\n",
    "            if idx is not None:\n",
    "                indices_left = X[:, idx] < thr\n",
    "                X_left, y_left = X[indices_left], y[indices_left]\n",
    "                X_right, y_right = X[~indices_left], y[~indices_left]\n",
    "                node.feature_index = idx\n",
    "                node.threshold = thr\n",
    "                node.left = self._grow_tree(X_left, y_left, depth + 1)\n",
    "                node.right = self._grow_tree(X_right, y_right, depth + 1)\n",
    "        return node\n",
    "\n",
    "    def _predict(self, inputs):\n",
    "        node = self.tree_\n",
    "        while node.left:\n",
    "            if inputs[node.feature_index] < node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.predicted_class\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    from sklearn.datasets import load_iris\n",
    "\n",
    "    dataset = load_iris()\n",
    "    X, y = dataset.data, dataset.target  # pylint: disable=no-member\n",
    "    clf = DecisionTreeClassifier(max_depth=1)\n",
    "    clf.fit(X, y)\n",
    "    print(clf.predict([[6.9, 3.1, 5.1, 2.3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Node object at 0x00000242C798C518>\n"
     ]
    }
   ],
   "source": [
    "print(clf.tree_.left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'c:\\\\users\\\\nadzi\\\\appdata\\\\local\\\\programs\\\\python\\\\python36\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\iris.csv'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_spltter(node,random_features):\n",
    "    '''this function recursively splits the data starting with the root node which its passed\n",
    "    untill the groups are homogenous or further splits result in empty nodes'''\n",
    "    #from IPython.core.debugger import Tracer; Tracer()() \n",
    "    #retrieve two groups from the passed which is root or a recursive call on itself\n",
    "    left_group,right_group = node['groups']\n",
    "    #delete the groups entry in original node\n",
    "    del node['groups']\n",
    "    #check if the groups of the node are empty\n",
    "    if left_group.empty or right_group.empty:\n",
    "        #combine as we will use original to predict\n",
    "        combined = pd.concat([left_group,right_group])\n",
    "        predicted_class = combined['class'].value_counts().index[0]\n",
    "        node['left']=node['right']=predicted_class\n",
    "        return\n",
    "    #check if the groups of the node are homogenous otherwise call recursive_spltter again\n",
    "    if single_gini_index(left_group) == 0:\n",
    "        predicted_class = left_group['class'].value_counts().index[0]\n",
    "        node['left'] = predicted_class\n",
    "    else :\n",
    "        node['left'] = find_best_split_point(left_group,random_features)\n",
    "        recursive_spltter(node['left'],random_features)\n",
    "\n",
    "    if single_gini_index(right_group) == 0:\n",
    "        predicted_class = right_group['class'].value_counts().index[0]\n",
    "        node['right'] = predicted_class\n",
    "    \n",
    "    else:\n",
    "        node['right'] = find_best_split_point(right_group,random_features)\n",
    "        recursive_spltter(node['right'],random_features)\n",
    "\n",
    "def find_best_split_point(passed_data, random_features):\n",
    "    '''find best split point iterating over range of values returned from the \n",
    "    get_range_to_split_on function and return a dictionary which functions as a node '''\n",
    "\n",
    "    \n",
    "     #intialise values for best split point\n",
    "    best_split_column = 'name'\n",
    "    best_split_value = 0\n",
    "    best_split_gini = 10\n",
    "    best_split_groups = None\n",
    "    \n",
    "    #iterate over columns and rows searching for best split point\n",
    "    for col_name in random_features:\n",
    "        # for split_value in splitpoints_dict[col_name]:\n",
    "        for index,row in passed_data.iterrows():\n",
    "            left_split, right_split = build_split(passed_data,col_name,row[col_name])\n",
    "            gini_score = multi_gini_index(left_split, right_split)\n",
    "\n",
    "            if gini_score < best_split_gini:\n",
    "                best_split_gini = gini_score\n",
    "                best_split_column = col_name\n",
    "                best_split_value = row[col_name]\n",
    "                best_split_groups = left_split, right_split\n",
    "    \n",
    "    \n",
    "    #print(best_split_column,best_split_gini)\n",
    "    return {'column_name': best_split_column,'dsplit_value':best_split_value,\n",
    "             'gini':best_split_gini, 'groups': best_split_groups}\n",
    "\n",
    "def build_split(data,column_to_split,split_value):\n",
    "    '''build 2 groups of data by splitting data on the column_to_split \n",
    "       at the split_value'''\n",
    "    left_split = data[data[column_to_split]<split_value]\n",
    "    right_split = data[data[column_to_split]>=split_value]\n",
    "    \n",
    "    return left_split,right_split\n",
    "\n",
    "    \n",
    "def multi_gini_index(group1,group2):\n",
    "    '''Calculate Gini Impurity, func expects to be passed \n",
    "       the 2 groups of data that are the result of a split'''\n",
    "    class_proportions_group1 = group1['class'].value_counts(normalize=True)    \n",
    "    class_proportions_group2 = group2['class'].value_counts(normalize=True)    \n",
    "\n",
    "    instance_proportion_group1 = len(group1)/(len(group1)+len(group2))\n",
    "    instance_proportion_group2 = len(group2)/(len(group1)+len(group2))\n",
    "\n",
    "    gini1 = (1 - class_proportions_group1.pow(2).sum())*(instance_proportion_group1)\n",
    "    gini2 = (1 - class_proportions_group2.pow(2).sum())*(instance_proportion_group2)\n",
    "    gini = gini1+gini2\n",
    "\n",
    "    return gini\n",
    "\n",
    "def single_gini_index(group):\n",
    "    '''Calculate Gini Impurity of a single group'''\n",
    "    class_proportions = group['class'].value_counts(normalize=True)    \n",
    "\n",
    "    gini = (1 - class_proportions.pow(2).sum())\n",
    "  \n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_attribute_subsets(subset, attribute_index):\n",
    "    attribute_subsets = {}\n",
    "    for sample in subset:\n",
    "        if sample[attribute_index] in attribute_subsets:\n",
    "            attribute_subsets[sample[attribute_index]].append(sample)\n",
    "        else:\n",
    "            attribute_subsets[sample[attribute_index]] = [sample]\n",
    "    return attribute_subsets\n",
    "\n",
    "\n",
    "def create_class_counts(subset):\n",
    "    class_counts = {}\n",
    "    for sample in subset:\n",
    "        if sample[class_index] in class_counts:\n",
    "            class_counts[sample[class_index]] += 1\n",
    "        else:\n",
    "            class_counts[sample[class_index]] = 1\n",
    "    return class_counts\n",
    "\n",
    "\n",
    "def get_most_common_class(subset):\n",
    "    max_class_count, most_common_class = -1, ''\n",
    "    class_counts = create_class_counts(subset)\n",
    "    for the_class, class_count in class_counts.items():\n",
    "        if class_count > max_class_count:\n",
    "            max_class_count, most_common_class = class_count, the_class\n",
    "    return most_common_class\n",
    "\n",
    "\n",
    "def calculate_entropy(subset):\n",
    "    class_counts = create_class_counts(subset)\n",
    "    subset_entropy = 0\n",
    "    for class_count in class_counts.values():\n",
    "        p_x = float(class_count) / len(subset)\n",
    "        subset_entropy -= p_x * math.log(p_x)\n",
    "    return subset_entropy\n",
    "\n",
    "\n",
    "# Aggregate all the attribute values\n",
    "attribute_values = [None] * num_cols\n",
    "for attribute_index in feature_indices:\n",
    "    attribute_values[attribute_index] = list(set([x[attribute_index] for x in dataset]))\n",
    "\n",
    "\n",
    "def build_tree(subset, unused_attributes, feature_subset_size):\n",
    "    # If all elements of subset are in the same class we can make this a leaf node\n",
    "    if len(set([x[class_index] for x in subset])) == 1:\n",
    "        return subset[0][class_index]\n",
    "\n",
    "    if len(unused_attributes) == 0:\n",
    "        # Get most common class\n",
    "        return get_most_common_class(subset)\n",
    "\n",
    "    # Apply the random forest technique of only considering a subset of features here\n",
    "    # but only if we have enough features to begin with.\n",
    "    feature_subset = unused_attributes\n",
    "    if len(feature_subset) > feature_subset_size:\n",
    "        feature_subset = random.sample(feature_subset, feature_subset_size)\n",
    "\n",
    "    min_index, min_entropy = 0, sys.maxsize\n",
    "    for attribute_index in feature_subset:\n",
    "        # Create subsets for that given attribute\n",
    "        attribute_subsets = create_attribute_subsets(subset, attribute_index)\n",
    "\n",
    "        # Calculate entropy of the generated subsets and weight them according to their size\n",
    "        entropy = 0\n",
    "        for attribute_subset in attribute_subsets.values():\n",
    "            entropy += float(len(attribute_subset) / len(subset)) * calculate_entropy(attribute_subset)\n",
    "\n",
    "        # Keep track of the split attribute with minimum entropy\n",
    "        if entropy < min_entropy:\n",
    "            min_index, min_entropy = attribute_index, entropy\n",
    "\n",
    "    # Copy list otherwise our passed feature_indices array will be modified.\n",
    "    unused_attributes = list(unused_attributes)\n",
    "    unused_attributes.remove(min_index)\n",
    "\n",
    "    # Reconstruct subsets of best attribute and use them to build subtrees recursively\n",
    "    # As a measure of feature importance we save the feature importance (information gain) here\n",
    "    attribute_subsets = create_attribute_subsets(subset, min_index)\n",
    "    tree = {'attribute_index': min_index, 'subtrees': {}, 'information_gain': calculate_entropy(subset) - min_entropy}\n",
    "\n",
    "    # Make sure to add for all values that exist in the training dataset labels\n",
    "    most_common_class = None\n",
    "    for attribute_value in attribute_values[min_index]:\n",
    "        if attribute_value in attribute_subsets and len(attribute_subsets[attribute_value]):\n",
    "            tree['subtrees'][attribute_value] = build_tree(attribute_subsets[attribute_value], unused_attributes,\n",
    "                                                           feature_subset_size)\n",
    "        else:\n",
    "            if not most_common_class:\n",
    "                most_common_class = get_most_common_class(subset)\n",
    "            tree['subtrees'][attribute_value] = most_common_class\n",
    "\n",
    "    return tree\n",
    "\n",
    "\n",
    "def classify(example, tree):\n",
    "    if isinstance(tree, str):\n",
    "        return tree\n",
    "    else:\n",
    "        return classify(example, tree['subtrees'][example[tree['attribute_index']]])\n",
    "\n",
    "# Counts predictions of all trees in the forest and selects the one with the highest count\n",
    "def majority_vote(example, trees):\n",
    "    votes = {}\n",
    "    for tree in trees:\n",
    "        prediction = classify(example, tree)\n",
    "        if prediction in votes:\n",
    "            votes[prediction] += 1\n",
    "        else:\n",
    "            votes[prediction] = 1\n",
    "    majority_prediction, max_vote = None, 0\n",
    "    for prediction, num_votes in votes.items():\n",
    "        if num_votes > max_vote:\n",
    "            majority_prediction, max_vote = prediction, num_votes\n",
    "    return majority_prediction\n",
    "\n",
    "\n",
    "# Translate attribute indices into human readable columns\n",
    "def pretty_tree(tree):\n",
    "    if isinstance(tree, str):\n",
    "        return tree\n",
    "    else:\n",
    "        pretty_subtrees = {}\n",
    "        for attribute_value, subtree in tree['subtrees'].items():\n",
    "            pretty_subtrees[attribute_value] = pretty_tree(subtree)\n",
    "        return { 'attribute_index' : header[tree['attribute_index']], 'subtrees' : pretty_subtrees}\n",
    "\n",
    "\n",
    "# Estimate importance of a feature by calculating the average information gain\n",
    "def get_feature_importance(tree, feature_importance):\n",
    "    if not isinstance(tree, str):\n",
    "        if header[tree['attribute_index']] in feature_importance:\n",
    "            feature_importance[header[tree['attribute_index']]].append(tree['information_gain'])\n",
    "        else:\n",
    "            feature_importance[header[tree['attribute_index']]] = [tree['information_gain']]\n",
    "        for subtree in tree['subtrees'].values():\n",
    "            get_feature_importance(subtree, feature_importance)\n",
    "\n",
    "\n",
    "# Feature importance of a forest is calculated by averaging the feature importances of the individual trees\n",
    "def get_feature_importance_for_forest(trees):\n",
    "    feature_importance = {}\n",
    "    for tree in trees:\n",
    "        get_feature_importance(tree, feature_importance)\n",
    "\n",
    "    # Average the information gains\n",
    "    for feature_name, importance in feature_importance.items():\n",
    "        feature_importance[feature_name] = float(sum(importance)) / len(importance)\n",
    "\n",
    "    # Finally sort feature importance in descending order\n",
    "    return sorted(feature_importance.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "\n",
    "# Use a single tree to test our ID3 algorithm.\n",
    "# We pass sys.maxsize so no random feature sampling is used.\n",
    "the_tree = build_tree(training, feature_indices, sys.maxsize)\n",
    "#print(pretty_tree(the_tree))\n",
    "\n",
    "num_classified_correctly = sum([classify(example, the_tree) ==\n",
    "                                example[class_index] for example in test])\n",
    "print('Single tree: Accuracy on test dataset: {:.2f}%'.format(float(100 * num_classified_correctly) / num_test_rows))\n",
    "\n",
    "\n",
    "# Perform bootstrap aggregating: Build trees from multiple sample sets\n",
    "trees = []\n",
    "for i in range(args.num_trees):\n",
    "    bootstrap_sample_set = [random.choice(training) for j in range(len(training))]\n",
    "    trees.append(build_tree(bootstrap_sample_set, feature_indices, sys.maxsize))\n",
    "\n",
    "num_classified_correctly = sum([majority_vote(example, trees) ==\n",
    "                                example[class_index] for example in test])\n",
    "print('Bagging: Accuracy on test dataset: {:.2f}%'.format(float(100 * num_classified_correctly) / num_test_rows))\n",
    "\n",
    "# Random forest: Additionally select subset of features\n",
    "trees = []\n",
    "for i in range(args.num_trees):\n",
    "    bootstrap_sample_set = [random.choice(training) for j in range(len(training))]\n",
    "    trees.append(build_tree(bootstrap_sample_set, feature_indices, args.feature_subset_size))\n",
    "\n",
    "num_classified_correctly = sum([majority_vote(example, trees) ==\n",
    "                                example[class_index] for example in test])\n",
    "print('Random forest: Accuracy on test dataset: {:.2f}%'.format(float(100 * num_classified_correctly) / num_test_rows))\n",
    "\n",
    "# Print out feature importances\n",
    "for rank, feature_importance in enumerate(get_feature_importance_for_forest(trees)):\n",
    "    print(str(rank + 1) + ': ' + str(feature_importance[0]) + ' {:.3}'.format(feature_importance[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and cleaning the data\n",
    "titanic = pd.read_csv(\"train.csv\")\n",
    "target = titanic[\"Survived\"]\n",
    "features = titanic.loc[:, [\"Pclass\", \"Sex\", \"Embarked\"]]\n",
    "features.Embarked.fillna(features.Embarked.mode()[0], inplace=True)\n",
    "features.Pclass = features.Pclass.map({1: \"1st\", 2: \"2nd\", 3: \"3rd\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows  12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "886      0            211536  13.0000   NaN        S  \n",
       "887      0            112053  30.0000   B42        S  \n",
       "888      2        W./C. 6607  23.4500   NaN        S  \n",
       "889      0            111369  30.0000  C148        C  \n",
       "890      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main class\n",
    "class Node:\n",
    "    def __init__(self, features, target):\n",
    "        self.left = None \n",
    "        self.right = None\n",
    "        self.features = features\n",
    "        self.target = target \n",
    "        self.feature_types = self.select_dtype()\n",
    "        \n",
    "    # Identiy feature types\n",
    "    def select_dtype(self):\n",
    "        feature_types = []\n",
    "        for item in self.features.columns:\n",
    "            if self.features[item].dtype != \"O\":\n",
    "                feature_types.append((item, \"numerical\")) \n",
    "            else:\n",
    "                if self.features[item].nunique() <= 2:\n",
    "                    feature_types.append((item, \"binary\"))\n",
    "                else:\n",
    "                    feature_types.append((item, \"multiclass\"))\n",
    "        return feature_types\n",
    "    \n",
    "    # Calculate gini impurity for each feature at a node\n",
    "    # Need to provide a default value as a fallback \n",
    "    @staticmethod\n",
    "    def gini_impurity_total(a=0, b=0, c=0, d=0):\n",
    "        total_elements = a + b + c + d\n",
    "        gini_1 = 1 - np.square(a/(a+b)) - np.square(b/(a+b))\n",
    "        gini_2 = 1 - np.square(c/(c+d)) - np.square(d/(c+d))\n",
    "        total_gini = ((a+b)/total_elements) * gini_1 + ((c+d)/total_elements) * gini_2\n",
    "        return total_gini \n",
    "    \n",
    "    @staticmethod\n",
    "    def gini_impurity(a=0, b=0):\n",
    "        return 1 - np.square(a/(a+b)) - np.square(b/(a+b))\n",
    "    \n",
    "    # Calculate gini for all feature combinations in categorical features\n",
    "    def calculate_gini(self, feature):\n",
    "        gini_node = []\n",
    "        combinations = []\n",
    "        \n",
    "        for i in range(1, self.features[feature].nunique()):\n",
    "            combinations = combinations + list(itertools.combinations(self.features[feature].unique(), i))\n",
    "            \n",
    "        for item in combinations:\n",
    "            t1 = self.target[self.features[feature].isin(item)] \n",
    "            t2 = self.target[~self.features[feature].isin(item)]\n",
    "            args = t1.value_counts().tolist() + t2.value_counts().tolist()\n",
    "            gini_node.append(Node.gini_impurity_total(*args))\n",
    "        \n",
    "        return gini_node, combinations # Return all the values\n",
    "     # Get the best gini values for each feature  \n",
    "    def evaluate_node(self):\n",
    "        gini_values = []\n",
    "        for feature in self.features.columns:\n",
    "            calculated_gini, combinations = self.calculate_gini(feature)\n",
    "            best_combination = combinations[np.argmin(calculated_gini)]\n",
    "            gini_values.append((feature, best_combination, np.min(calculated_gini)))\n",
    "        return gini_values\n",
    "                \n",
    "    # Inserting a new node based on the decision criteria\n",
    "    def insert_node(self):\n",
    "        gini_values = self.evaluate_node()\n",
    "        values = [item[2] for item in gini_values]\n",
    "        node_gini =  Node.gini_impurity(*self.target.value_counts().tolist())\n",
    "        \n",
    "         # Terminate the branch if current gini is better or no features to split\n",
    "        if node_gini < np.min(values): \n",
    "            print(\"terminating the branch\")\n",
    "            self.left = None\n",
    "            self.right = None \n",
    "        else:\n",
    "            best_feature = gini_values[np.argmin(values)][0]\n",
    "            best_combination = gini_values[np.argmin(values)][1]\n",
    "            print(f\"Creating a new branch using {best_feature} and {best_combination}\")\n",
    "\n",
    "            left_features = self.features[self.features[best_feature].isin(best_combination)]\n",
    "            left_features.drop([best_feature], axis=1, inplace=True)\n",
    "            left_target = self.target[self.features[best_feature].isin(best_combination)]\n",
    "            if list(left_features.columns) == []:\n",
    "                self.left = None \n",
    "                self.right = None\n",
    "            else:\n",
    "                self.left = Node(left_features, left_target)\n",
    "                self.left.insert_node()\n",
    "\n",
    "            right_features = self.features[~self.features[best_feature].isin(best_combination)]\n",
    "            right_target = self.target[~self.features[best_feature].isin(best_combination)]\n",
    "            right_features.drop([best_feature], axis=1, inplace=True)\n",
    "            if list(right_features.columns) == []:\n",
    "                self.left = None \n",
    "                self.right = None\n",
    "            else:\n",
    "                self.right = Node(right_features, right_target)\n",
    "                self.right.insert_node()\n",
    "\n",
    "# Creating the root node with the full dataset \n",
    "root = Node(features, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new branch using Sex and ('male',)\n",
      "Creating a new branch using Pclass and ('1st',)\n",
      "Creating a new branch using Embarked and ('Q',)\n",
      "Creating a new branch using Embarked and ('C',)\n",
      "Creating a new branch using Pclass and ('3rd',)\n",
      "Creating a new branch using Embarked and ('S',)\n",
      "Creating a new branch using Embarked and ('Q',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nadzi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\frame.py:4174: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "root.insert_node()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
